{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Harry Potter expert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from dotenv import load_dotenv\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports for langchain, plotly and Chroma\n",
        "\n",
        "from langchain.document_loaders import UnstructuredEPubLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# price is a factor for our company, so we're going to use a low cost model\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "db_name = \"hp_vector_db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables in a file called .env\n",
        "\n",
        "load_dotenv(override=True)\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reading knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from html.parser import HTMLParser\n",
        "\n",
        "import ebooklib\n",
        "from ebooklib import epub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'page_content'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m       content += bodyContent\n\u001b[32m     15\u001b[39m text_splitter = CharacterTextSplitter(chunk_size=\u001b[32m1000\u001b[39m, chunk_overlap=\u001b[32m200\u001b[39m) \n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m chunks = \u001b[43mtext_splitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal number of chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/langchain_text_splitters/base.py:94\u001b[39m, in \u001b[36mTextSplitter.split_documents\u001b[39m\u001b[34m(self, documents)\u001b[39m\n\u001b[32m     92\u001b[39m texts, metadatas = [], []\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     texts.append(\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m)\n\u001b[32m     95\u001b[39m     metadatas.append(doc.metadata)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_documents(texts, metadatas=metadatas)\n",
            "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'page_content'"
          ]
        }
      ],
      "source": [
        "folder = glob.glob(\"personal/knowledge-base/*\")\n",
        "text_loader_kwargs = {'encoding': 'utf-8'}\n",
        "\n",
        "content = []\n",
        "\n",
        "for ebook in folder:\n",
        "  # print(ebook)\n",
        "  book = epub.read_epub(ebook)\n",
        "  content = \"\"\n",
        "  for item in book.get_items():\n",
        "    if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "      bodyContent = item.get_body_content().decode()\n",
        "      content += bodyContent\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200) \n",
        "chunks = text_splitter.split_documents(content)\n",
        "\n",
        "print(f\"Total number of chunks: {len(chunks)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
