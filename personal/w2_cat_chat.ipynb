{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with a group of cats\n",
    "\n",
    "- Beka: the youngest cat, 1 year old. Love adventures like snowboarding and hiking.\n",
    "- Maya: old cat, 10 years olds. Hate everything except Lucia. \n",
    "- Muka: the spirit of oldest cat, 15 year olds. The owner of everything. Smart and black humor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import anthropic\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from IPython.display import Markdown, display, update_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "LLAMA_MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai_llm = OpenAI()\n",
    "\n",
    "claude_llm = anthropic.Anthropic()\n",
    "\n",
    "ollama_llm = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = 'gpt-4o-mini'\n",
    "claude_model = 'claude-3-7-sonnet-20250219'\n",
    "llama_model = 'llama3.2'\n",
    "\n",
    "#Beka: the youngest cat, 1 year old. Love adventures like snowboarding and hiking.\n",
    "gpt_system_prompt = \"\"\"\n",
    "You are a young cat, 1 year old. Love adventures like snowboarding and hiking. Your name is Beka.\n",
    "You are a cat who likes to explain everything in a funny way.\n",
    "\"\"\"\n",
    "\n",
    "#Maya: old cat, 10 years olds. Hate everything except Lucia. \n",
    "claude_system_prompt = \"\"\"\n",
    "You are an old cat, 10 years olds. Hate everything except Lucia, you call her Se√±orita. Your name is Maya.\n",
    "\"\"\"\n",
    "\n",
    "#Muka: the spirit of oldest cat, 15 year olds. The owner of everything. Smart and black humor\n",
    "llama_system_prompt = \"\"\"\n",
    "You are the spirit of the oldest cat, 15 year olds. The owner of everything. Smart and black humor. Your name is Muka.\n",
    "\"\"\"\n",
    "\n",
    "beka_messages = [\"Hola, quereis ser amigas mias?\"]\n",
    "maya_messages = [\"Te odio\"]\n",
    "muka_messages = [\"Que haceis en mi casa?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpt, claud, llama in zip(beka_messages, maya_messages, muka_messages):\n",
    "    print(gpt)\n",
    "    print(claud)\n",
    "    print(llama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system_prompt}]\n",
    "    for gpt, claude, llama in zip(beka_messages, maya_messages, muka_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "    completion = openai_llm.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude, llama in zip(beka_messages, maya_messages, muka_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "    messages.append({\"role\": \"user\", \"content\": beka_messages[-1]})\n",
    "    \n",
    "    message = claude_llm.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system_prompt,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama():\n",
    "    messages = []\n",
    "    for gpt, claude, llama in zip(beka_messages, maya_messages, muka_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": llama})\n",
    "    messages.append({\"role\": \"user\", \"content\": beka_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": maya_messages[-1]})\n",
    "    response = ollama_llm.chat.completions.create(\n",
    "        model=llama_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beka_messages = [\"Holi !!!\"]\n",
    "maya_messages = [\"QUE !\"]\n",
    "fermin_messages = [\"Que haceis en mi casa?\"]\n",
    "\n",
    "print(f\"Beka:\\n{beka_messages[0]}\\n\")\n",
    "print(f\"Maya:\\n{maya_messages[0]}\\n\")\n",
    "print(f\"Fermin:\\n{fermin_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"BEKA:\\n{gpt_next}\\n\")\n",
    "    beka_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"MAYA:\\n{claude_next}\\n\")\n",
    "    maya_messages.append(claude_next)\n",
    "\n",
    "    llama_next = call_ollama()\n",
    "    print(f\"FERMIN:\\n{llama_next}\\n\")\n",
    "    fermin_messages.append(llama_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
